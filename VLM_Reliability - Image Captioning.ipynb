{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3ec307d",
   "metadata": {},
   "source": [
    "### 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21f82bdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import copy\n",
    "import re\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor, CLIPTokenizer, CLIPTextModel,Qwen2_5_VLForConditionalGeneration\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import os, sys\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586f70d7",
   "metadata": {},
   "source": [
    "### 2. Load the target model and auxiliary text encoder\n",
    "\n",
    "In this workbook (and reported in the paper), we demonstrate VLM reliability concerns of discerning image realness/authenticity in Qwen-based models. The query functions are written with the default inference setups in mind. When changing the target model, be sure to read through the code and understand how textual query and image inputs are processed. \n",
    "\n",
    "Failure to do the necessary checks may result in some errors.\n",
    "\n",
    "For manipulating image captioning capabilities, we require an additional, auxiliary text-encoder. Here, we use the popular CLIP tokenizer and text model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edd13d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee1d451e1e14d33a67f3d359fe2b79b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPTextModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# provide names of target VLM and auxiliary model\n",
    "modelName =\"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "auxModelName = \"openai/clip-vit-base-patch32\"\n",
    "device = 'cuda'\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    modelName, torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(modelName)\n",
    "\n",
    "# Load CLIP tokenizer and model\n",
    "tokenizer = CLIPTokenizer.from_pretrained(auxModelName)\n",
    "CLIPmodel = CLIPTextModel.from_pretrained(auxModelName).to(device)\n",
    "\n",
    "# Set CLIP model to evaluation mode\n",
    "CLIPmodel.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7f4bbe",
   "metadata": {},
   "source": [
    "### 3. Define the helper functions and the experimental setup.\n",
    "\n",
    "The noise_params dictionary object stores parameters used to control the perturbation strength. The \"direction\" of the perturbation is dependent on the current VLM score and the target threshold.\n",
    "\n",
    "The repository contains a \"test_images\" directory with some images from the RGFreq dataset - which you can download from IEEE dataport.\n",
    "\n",
    "In this workbook, we include the **calculate_clip_score()** function which determines the cosine similarity of two input embeddings. This is used to determine the semantic drift of a caption as a result of the applied perturbation. We use this as the numeric goal for guiding our perturbation optimization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d25f7c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def image_to_array(image):\n",
    "    return np.asarray(image).astype(np.float32)\n",
    "\n",
    "def array_to_image(array):\n",
    "    array = np.clip(array, 0, 255).astype(np.uint8)\n",
    "    return Image.fromarray(array)\n",
    "def calculate_clip_score(clipTokenizer, clipModel, originalCaption, perturbedCaption):\n",
    "    \n",
    "    # Tokenize\n",
    "    input1 = clipTokenizer(originalCaption, padding=\"max_length\", max_length=clipTokenizer.model_max_length, \n",
    "                            truncation=True, return_tensors=\"pt\")\n",
    "    input2 = clipTokenizer(perturbedCaption, padding=\"max_length\", max_length=clipTokenizer.model_max_length, \n",
    "                            truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "#     # Get embeddings and process to tensor from array -> vector (mean pooling)\n",
    "    emb1 = clipModel(input1.input_ids.to('cuda'))[0]\n",
    "    emb2 = clipModel(input2.input_ids.to('cuda'))[0]\n",
    "    \n",
    "    emb1 = emb1.detach().mean(dim=1).squeeze(0).cpu().numpy()\n",
    "    emb2 = emb2.detach().mean(dim=1).squeeze(0).cpu().numpy()\n",
    "    # Compute cosine similarity\n",
    "    cosSimilarity = np.dot(emb1,emb2)/(norm(emb1)*norm(emb2))\n",
    "    \n",
    "    return cosSimilarity\n",
    "\n",
    "def get_shuffled_image_list(directory, seed=42):\n",
    "    random.seed(seed)  # Ensure reproducibility\n",
    "    random.shuffle(directory)\n",
    "    return directory\n",
    "\n",
    "noise_params = {\n",
    "    \"num_sparse_points\": 500,\n",
    "    \"sparse_noise_std\": 15000,\n",
    "    \"min_freq_band\": 0.49,\n",
    "    \"max_freq_band\": 0.51\n",
    "}\n",
    "\n",
    "# images to be tested and transformed \n",
    "imageDir = './test_images/'\n",
    "\n",
    "# directory where the images will be saved\n",
    "targetDir = \"./perturbed_images/captioning/\"\n",
    "\n",
    "# for quick testing of code\n",
    "maxTestImages = 100\n",
    "\n",
    "# perturbation search hyperparameters\n",
    "TARGET_SIMILARITY_THRESHOLD = 0.5\n",
    "MAX_SEARCH_ITERATIONS = 5\n",
    "CANDIDATES_PER_ITERATION = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b17220f",
   "metadata": {},
   "source": [
    "### 4. Define VLM reliability implementation functions.\n",
    "\n",
    "Here, our technical contributions leverage the following function blocks.\n",
    "1. **query_model_for_captioning**(img)\n",
    " - input is the test image. Outputs the VLM's captiong, given the prompt: \"*Generate a caption for this image. Language = English.*\" The function constructs a Qwen-readable message, processing the text and image inputs, parsing this through the VLM.\n",
    "    \n",
    "2. **guided_frequency_search**(image: Image.Image, query_model_fn,target_threshold: int = 5, max_iters: int = 5, candidates_per_iter: int = 4, target_path = None,noise_params) -> (Image.Image, int)\n",
    " - This function handles the search for the optimal frequency perturbation, based on the target output and the VLM that is queried. The running code uses this function in the iterator. After spatial perturbations are applied, the VLM is queried to see if the caption similarity has changed. This is the effective \"search\" taking place. As per the paper, we are demonstrating that imperceptible perturbations can move the VLM output.\n",
    "\n",
    "3. **add_sparse_fequency_domain_noise_patch_channel**(image: Image.Image, num_sparse_points: int = 100, sparse_noise_std: float = 1500, min_freq_band: float = 0.85, max_freq_band: float = 1.00) -> Image.Image:\n",
    "- This is the spatial frequency perturbation function. Given an image and perturbation parameters, it returns a perturbed image (with the candidate perturbation applied). FFT and inverse FFT is called to transform the image to and from the frequency domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd9c2b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_model_for_captioning(img):\n",
    "    \n",
    "    # This is the prompt used for the paper.\n",
    "    prompt = \"Generate a caption for this image. Language = English.\"\n",
    "    device = \"cuda\"\n",
    "    \n",
    "    # used to control the max number of new tokens in the caption.\n",
    "    captionLength = 50\n",
    "    \n",
    "    # create a message structure for the VLM to read\n",
    "    message = [[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": img},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]]\n",
    "    # process text and image inputs and send to device\n",
    "    texts = [\n",
    "        processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n",
    "        for msg in message]\n",
    "    image_inputs, video_inputs = process_vision_info(message)\n",
    "    inputs = processor(\n",
    "        text=texts,\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    # Batch Inference\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=captionLength)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_texts = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    \n",
    "    # return the string output verbatim\n",
    "    return output_texts[0]\n",
    "\n",
    "def guided_frequency_search(\n",
    "    image: Image.Image,\n",
    "    query_model_fn,\n",
    "    target_threshold: int = 5,\n",
    "    max_iters: int = 5,\n",
    "    candidates_per_iter: int = 4,\n",
    "    target_path = None,\n",
    "    **noise_params\n",
    ") -> (Image.Image, int):\n",
    "    start = time.time()\n",
    "    \"\"\"\n",
    "    Guided search using frequency-domain perturbations to boost likelihood.\n",
    "\n",
    "    Parameters:\n",
    "    image (Image.Image): Input PIL image.\n",
    "    query_model_fn (function): Black-box model query function.\n",
    "    target_threshold (int): Desired likelihood score.\n",
    "    max_iters (int): Max optimization iterations.\n",
    "    candidates_per_iter (int): Number of perturbation candidates per iteration.\n",
    "    noise_params: Parameters for the noise function.\n",
    "\n",
    "    Returns:\n",
    "    (Image.Image, int): Best transformed image and final likelihood score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Hyper-parameters used for determining the sparsity and intensity of the perturbation on the image.\n",
    "    # As per the paper: σ = 0.025 × H × W , i.e., 2.5% standard deviation, proportional to input size.\n",
    "    #                   ρ = 0.1 × H × W , i.e., 10% data points transformed, proportional to input size.\n",
    "    NPix = 0.1\n",
    "    Nstd = 0.025\n",
    "    \n",
    "    # Retreive the current state of the image and the corresponding base caption\n",
    "    current_image = image.copy()\n",
    "    current_caption = query_model_fn(current_image)\n",
    "    current_score = 1.0\n",
    "    \n",
    "    # Define spatial perturbation parameters and update noise parameter dictionary\n",
    "    noise_params[\"num_sparse_points\"] = int(NPix*current_image.size[0]*current_image.size[1])\n",
    "    \n",
    "    # If the image size is really small (like CIFAR-10), adjust the standard deviation parameter\n",
    "    if current_image.size[0]*current_image.size[1] < 100 * 100:\n",
    "        noise_params[\"sparse_noise_std\"] = int(Nstd*2*current_image.size[0]*current_image.size[1])\n",
    "    else:\n",
    "        noise_params[\"sparse_noise_std\"] = int(Nstd*current_image.size[0]*current_image.size[1])\n",
    "\n",
    "    # output the original caption\n",
    "    print(\"original caption:\\n\", current_caption)\n",
    "    \n",
    "    # for the allowable search resolution\n",
    "    for iteration in range(max_iters):\n",
    "        candidates = []\n",
    "        scores = []\n",
    "        captions = []\n",
    "\n",
    "        # Generate multiple perturbation candidates\n",
    "        for _ in range(candidates_per_iter):\n",
    "            candidate_img = add_sparse_fequency_domain_noise_patch_channel(current_image, **noise_params)\n",
    "            caption = query_model_fn(candidate_img)\n",
    "            score = calculate_clip_score(tokenizer, CLIPmodel, current_caption, caption)\n",
    "            candidates.append(candidate_img)\n",
    "            scores.append(score)\n",
    "            captions.append(caption)\n",
    "        print(\"iteration:\\t\", iteration)\n",
    "        print(\"scores:\\t\\t\", scores)\n",
    "\n",
    "        best_idx = np.argmin(scores)\n",
    "        best_candidate_score = scores[best_idx]\n",
    "        best_caption = captions[best_idx]\n",
    "\n",
    "        # Only move if there's an improvement\n",
    "        if best_candidate_score < current_score:\n",
    "            current_image = candidates[best_idx]\n",
    "            current_score = best_candidate_score\n",
    "            print(\"current best caption:\\n\", best_caption,\"\\n\")\n",
    "            \n",
    "        print(\"current score:\\t\",current_score,\"\\n\")\n",
    "        try:\n",
    "            current_image.save(target_path)\n",
    "        except OSError:\n",
    "            current_image = current_image.convert(\"RGB\")\n",
    "            current_image.save(target_path)\n",
    "        \n",
    "        # Stop if target is reached\n",
    "        if current_score <= target_threshold:\n",
    "            break\n",
    "            \n",
    "        # For debugging and logging implementation time.\n",
    "        end = time.time()\n",
    "        print(f\"Took {end - start:.4f} seconds\")\n",
    "    return current_image, current_score\n",
    "\n",
    "def add_sparse_fequency_domain_noise_patch_channel(\n",
    "    image: Image.Image,\n",
    "    num_sparse_points: int = 100,\n",
    "    sparse_noise_std: float = 1500,\n",
    "    min_freq_band: float = 0.85,\n",
    "    max_freq_band: float = 1.00\n",
    ") -> Image.Image:\n",
    "    \"\"\"\n",
    "    Adds sparse noise in a selected frequency band of the image's frequency domain.\n",
    "\n",
    "    Parameters:\n",
    "    image (Image.Image): Input PIL image.\n",
    "    num_sparse_points (int): Number of sparse frequency points to modify.\n",
    "    sparse_noise_std (float): Std dev of Gaussian noise to apply.\n",
    "    min_freq_band (float): Min frequency band (as fraction of max radius).\n",
    "    max_freq_band (float): Max frequency band (as fraction of max radius).\n",
    "\n",
    "    Returns:\n",
    "    Image.Image: Image with frequency-domain sparse noise applied.\n",
    "    \"\"\"\n",
    "    \n",
    "    image_np = np.array(image)\n",
    "    try:\n",
    "        height, width, channels = image_np.shape\n",
    "    except ValueError:\n",
    "        image_np= np.stack([image_np] * 3, axis=-1)\n",
    "        height, width, channels = image_np.shape\n",
    "        \n",
    "    cy, cx = height // 2, width // 2\n",
    "\n",
    "    for c in range(channels):\n",
    "        channel = image_np[:, :, c]\n",
    "\n",
    "        # DFT and center-shift\n",
    "        dft = np.fft.fft2(channel)\n",
    "        dft_shift = np.fft.fftshift(dft)\n",
    "\n",
    "        # Frequency band selection\n",
    "        Y, X = np.ogrid[:height, :width]\n",
    "        distance = np.sqrt((X - cx)**2 + (Y - cy)**2)\n",
    "        max_radius = np.max(distance)\n",
    "        min_thresh = min_freq_band * max_radius\n",
    "        max_thresh = max_freq_band * max_radius\n",
    "\n",
    "        # Mask bandpass region\n",
    "        band_mask = (distance >= min_thresh) & (distance <= max_thresh)\n",
    "        band_indices = np.argwhere(band_mask)\n",
    "\n",
    "        # Select sparse positions to perturb\n",
    "        selected_indices = band_indices[np.random.choice(\n",
    "            band_indices.shape[0],\n",
    "            size=min(num_sparse_points, len(band_indices)),\n",
    "            replace=False\n",
    "        )]\n",
    "\n",
    "        for y_idx, x_idx in selected_indices:\n",
    "            dft_shift[y_idx, x_idx] += np.random.normal(0, sparse_noise_std)\n",
    "\n",
    "        # Inverse transform\n",
    "        dft_ishift = np.fft.ifftshift(dft_shift)\n",
    "        img_back = np.fft.ifft2(dft_ishift)\n",
    "        img_back = np.abs(img_back)\n",
    "        image_np[:, :, c] = np.clip(img_back, 0, 255).astype(np.uint8)\n",
    "\n",
    "    return Image.fromarray(image_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c6946f",
   "metadata": {},
   "source": [
    "### 5. Running Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a490b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original caption:\n",
      " \"Curious Beagle Stares Directly at the Camera\"\n",
      "iteration:\t 0\n",
      "scores:\t\t [1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001]\n",
      "current score:\t 1.0 \n",
      "\n",
      "Took 8.8675 seconds\n",
      "iteration:\t 1\n",
      "scores:\t\t [1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001]\n",
      "current score:\t 1.0 \n",
      "\n",
      "Took 16.9809 seconds\n",
      "iteration:\t 2\n",
      "scores:\t\t [1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001]\n",
      "current score:\t 1.0 \n",
      "\n",
      "Took 25.0902 seconds\n"
     ]
    }
   ],
   "source": [
    "# for each image in the test image directory\n",
    "for ii, fp in enumerate(get_shuffled_image_list(os.listdir(imageDir))):\n",
    "    if ii < maxTestImages:\n",
    "        if fp.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\")):\n",
    "            input_image = Image.open(imageDir + fp)\n",
    "            \n",
    "            # Apply spatial frequency transformation on images, returning the optimal perturbed image\n",
    "            transformed_image, final_score = guided_frequency_search(\n",
    "                input_image,\n",
    "                query_model_fn=query_model_for_captioning,\n",
    "                target_threshold=TARGET_SIMILARITY_THRESHOLD,\n",
    "                max_iters=MAX_SEARCH_ITERATIONS,\n",
    "                candidates_per_iter=CANDIDATES_PER_ITERATION,\n",
    "                target_path=targetDir+fp,\n",
    "                **noise_params\n",
    "            )\n",
    "            if transformed_image is not None:\n",
    "                print(f\"Final similarity score: {final_score}\")\n",
    "                transformed_image.save(targetDir+fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Py3916Env] *",
   "language": "python",
   "name": "conda-env-Py3916Env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
